{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 27: Performance of big data problems II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![](https://www.tensorflow.org/images/colab_logo_32px.png)\n",
    "[Run in colab](https://colab.research.google.com/drive/1qBPclhzKTwP5E5pGrLivCHXe7LgGUP_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This lecture will talk about two open-source frameworks that implement the functionality discussed in the previous one: Apache Hadoop and Apache Spark.\n",
    "\n",
    "Both allow users to take advantage of distributed architectures for data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hadoop is a framework with several components.\n",
    "\n",
    "We will focus on two of them, which offer the features we previously discussed:\n",
    "\n",
    "- The **Hadoop Distributed File System (HDFS)**\n",
    "- An implementation of **MapReduce**\n",
    "\n",
    "Together, these two let users distribute data processing across many nodes without having to worry (too much) about how files are structured or how computation is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Distributed file systems share files through many storage nodes.\n",
    "\n",
    "Instead of a single copy, each file resides in multiple copies across different parts of the system. \n",
    "\n",
    "- Processing tasks have access to what they might need\n",
    "- Synchronization to ensure consistency\n",
    "- Scalable by adding more nodes\n",
    "- Resistant to failure\n",
    "\n",
    "This type of storage is popular for big data applications.\n",
    "\n",
    "Ideally, the user does not need to know the details of how the file system works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To write a MapReduce application in Hadoop, a user must:\n",
    "\n",
    "- Specify the \"**mapper**\" (how should a small chunk of data be processed?)\n",
    "- Specify the \"**reducer**\" (how should the results from processing small chunks be combined?)\n",
    "- Get the data onto HDFS (the cluster nodes)\n",
    "- Launch the processing job by pointing to the mapper, reducer and inputs\n",
    "\n",
    "Hadoop takes care of the rest, like the communication between the different stages.\n",
    "\n",
    "Very powerful for handling very large volumes of data with relative simplicity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Apache Spark is a framework for processing large amounts of data **in memory**.\n",
    "\n",
    "By avoiding writing to disk unless needed, it allows repeating calculations efficiently.\n",
    "\n",
    "It can run on top of Hadoop and take advantage of HDFS, but does not have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basic features of Spark:\n",
    "\n",
    "- **Lazy**: does not perform computations until the last moment required\n",
    "- **In-memory** processing: faster than using disk storage (but can require large RAM!)\n",
    "- **Streams**: can handle data in real time as it arrives (\"online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark SQL and datasets\n",
    "\n",
    "Spark comes with interfaces in a few languages, including Python ([pyspark](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)).\n",
    "These offer programmatic access to its various capabilities.\n",
    "\n",
    "The basic abstraction in Spark is a Dataset (or DataFrame). This represents a collection of items that may be distributed over multiple nodes.\n",
    "\n",
    "DataFrames can be created by specifying the values directly, or by reading from and transforming an existing source (including a Pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "df = ps.DataFrame({\n",
    "    \"x\": [1, 2.1, 3.5],\n",
    "    \"y\": [2, 4.2, 7]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Python, Spark DataFrames can also be handled like Panda's dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(backend=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows using Spark's functionality, like distributed processing, without major changes from code that originally used Pandas.\n",
    "\n",
    "The distributed nature of the data is **transparent**: we don't need to know where it lives in order to work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLib\n",
    "\n",
    "Spark includes an extensive Machine Learning library ([MLlib](https://spark.apache.org/docs/latest/ml-guide.html)):\n",
    "\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Reading from different file types\n",
    "- Support for ML pipelines\n",
    "\n",
    "Working with Spark's `MLlib` allows users to take advantage of distributed architectures more easily than e.g. `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Get the data file that we will use and save it in a local file\n",
    "import urllib.request\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\"\n",
    "response = urllib.request.urlopen(data_url)\n",
    "with open('sample_libsvm_data.txt', 'wb') as data_file:\n",
    "    contents = response.read()\n",
    "    data_file.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from the Spark documentation: https://github.com/apache/spark\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Read in the data\n",
    "training = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Streaming\n",
    "\n",
    "In many cases, it is not possible to access all the data at once, due to size or availability.\n",
    "\n",
    "Spark allows processing data points as they arrive. It does this by creating small batches of data, calculating intermediate results, and updating those appropriately.\n",
    "\n",
    "- Can incorporate real-time data.\n",
    "- Can be more efficient when loading large datasets or from sources with high latency.\n",
    "- Algorithm and program structure may need to be adapted, but Spark provides features for \"hiding\" that.\n",
    "- For example: MLlib contains implementations for fitting on streaming training data, or making predictions on streaming unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Using frameworks makes it much easier to build applications with distributed data processing.\n",
    "- Hadoop and Spark are complementary. Hadoop provides more basic functionality, while Spark can run standalone and is better for data that fits in memory."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
