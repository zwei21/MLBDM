{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 14: Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![](https://www.tensorflow.org/images/colab_logo_32px.png)\n",
    "[Run in colab](https://colab.research.google.com/drive/1CxeEInBOmkXyYx-vBjKzYCh8Uaoccgsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:27.811811Z",
     "iopub.status.busy": "2022-02-14T21:50:27.811273Z",
     "iopub.status.idle": "2022-02-14T21:50:27.819988Z",
     "shell.execute_reply": "2022-02-14T21:50:27.819430Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2022-02-14 21:50:27\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Version: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:27.824301Z",
     "iopub.status.busy": "2022-02-14T21:50:27.823912Z",
     "iopub.status.idle": "2022-02-14T21:50:28.629554Z",
     "shell.execute_reply": "2022-02-14T21:50:28.628910Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_state(seed=42):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Visual cortex\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/eye_rods_cones.jpg\" width=\"700px\"/>\n",
    "\n",
    "[[Image source](https://www.blueconemonochromacy.org/wp-content/uploads/2011/02/1_EN.jpg)]\n",
    "\n",
    "\n",
    "Neurons in the visual cortex have a small local receptive field, i.e. only react to limited region of visual field.\n",
    "\n",
    "Receptive fields of neurons overlap and together cover full visual field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Architecture of the visual cortex\n",
    "\n",
    "Some neurons only sensitive to stimuli of certain structure, e.g. horizonal lines.\n",
    "\n",
    "Some neurons have larger receptive fields, and are sensitive to stimuli that are combinations of lower-level patterns.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/localFov.jpeg\" width=\"700px\"/>\n",
    "\n",
    "[Credit: Geron]\n",
    "\n",
    "Corresponding architecture can detect complex patterns in full visual field, which inspires design of convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convolution\n",
    "\n",
    "Core building block of CNNs is convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "###  Graphical description\n",
    "\n",
    "Convolution involves passing a filter (kernel) over an image and taking the sum of the product of terms for all positions.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution-operation-14.png\" width=\"700px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution-operation-24.png\"  width=\"700px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Animation of convolution\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution.gif\"  width=\"700px\"/>\n",
    "\n",
    "[[Animation source](https://miro.medium.com/max/1000/1*GcI7G-JLAQiEoCON7xFbhg.gif)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "###  Mathematical description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution output is given by\n",
    "\\begin{align*}\n",
    "z_{i,j} = \\sum_{u,v} x_{u,v} w_{u-i,v-j},\n",
    "\\end{align*}\n",
    "where $x$ is the input image, $w$ is the filter (kernel) and $i$ ($u$) and $j$ ($v$) denote row and column indices, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the kernel is not reflected as is typical in the usual mathematical definition of convolution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Advantages\n",
    "\n",
    "- Localisation: Capture local structure.\n",
    "- Efficiency: weight sharing results in dramatic reduction in number of weights (parameters) compared to fully-connected neural network.\n",
    "- Translational equivariance: Feature space behavies \"nicely\" under a translation of the input.  In a sense, learn general features rather than features for all locations of image.\n",
    "- Composition: Can compose convolutions to extract more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network layers using convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/geron_convolutional_layers.png\"  width=\"700px\"/>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows the network to focus on small low-level features in the first layer and use these to constuct higher-level features in next layer, and so on.  \n",
    "\n",
    "Results in a hierarchical representation that is common in images of the real-world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Filters and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters pull out corresponding features from image.  \n",
    "\n",
    "Output of convolutional layer is typically called a *feature map*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/geron_feature_maps.png\"  width=\"700px\"/>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, effective filters to extract representative features are learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "Nominally, convolution results in an output image that is *smaller* than the input.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution-operation-24.png\"  width=\"700px\"/>\n",
    "\n",
    "Various modifications to the nominal convolution are often considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Animation of convolution (no padding, no stride)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/conv_no_padding_no_strides.gif\"  width=\"500px\"/>\n",
    "\n",
    "[[Animation source](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_no_strides.gif)]\n",
    "\n",
    "Blue pixels denote the input image, grey the convolutional kernel as it moves over the image, and green pixels denote the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Padding often introduced to control output size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Animation of convolution (padding, no stride)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/conv_arbitrary_padding_no_strides.gif\"  width=\"500px\">\n",
    "\n",
    "[[Animation source](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/arbitrary_padding_no_strides.gif)]\n",
    "\n",
    "Uncoloured pixels denote padding with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation of convolution (no padding, stride)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/conv_no_padding_strides.gif\" width=\"500px\"/>\n",
    "\n",
    "[[Animation source](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/no_padding_strides.gif)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Padding and stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation of convolution (padding, stride)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/conv_padding_strides.gif\"  width=\"500px\"/>\n",
    "\n",
    "[[Animation source](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/padding_strides.gif)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple input channels\n",
    "\n",
    "Input images often have multiple channels, e.g. red, green and blue colour channels.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/rgb_2.jpg\"  width=\"700px\"/>\n",
    "\n",
    "[[Image source](https://code.tutsplus.com/tutorials/create-a-retro-crt-distortion-effect-using-rgb-shifting--active-3359)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Support multiple input channels by defining a filter for each channel and summing result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution-operation-on-volume5.png\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Multiple input and output channels\n",
    "\n",
    "Add a set of filters for each desired output channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/convolution-with-multiple-filters2.png\"  width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Mathematical description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution output is given by\n",
    "\\begin{align*}\n",
    "z_{i,j,k_\\text{out}} = \\sum_{u,v,k_\\text{in}} x_{u,v,k_\\text{in}} w_{u-i,v-j,k_\\text{in},k_\\text{out}},\n",
    "\\end{align*}\n",
    "where $x$ is the input image, $w$ is the filter (kernel) and $i$ ($u$) and $j$ ($v$) denote row and column indices, respectively.  The input channel index is denoted $k_\\text{in}$ and the output channel index $k_\\text{out}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Notice that the filter is now 4-dimensional.  We quickly add a lot of additional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-linear activations\n",
    "\n",
    "Convolutions usually followed by pointwise activation functions to introduce non-linearity (cf. fully-connected neural networks).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/activation_func.png\"  width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Full convolutional layer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/one-convolution-layer1.png\"  width=\"900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers are used to downsample to reduce the computational load, memory usage and number of parameters.\n",
    "\n",
    "Retains equivariance to large translation and can introduce invariance to small translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Max pooling\n",
    "\n",
    "Take maximum over parent cell size, translating cell over image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/pooling-layer3_max.png\"  width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Average pooling\n",
    "\n",
    "Take average over parent cell size, translating cell over image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/pooling-layer3_average.png\"  width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Multiple input channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering multiple input channels, pooling is performed separately for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/pooling-on-volume1.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Stacking multiple feature maps\n",
    "\n",
    "Convolutional layers typically stacked with multiple input and output channels, leading to multiple feature maps.\n",
    "\n",
    "Neuron's receptor field extends across all previous layers' feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/stacked.jpeg\"  width=\"700px\"/>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Basic CNN architecture\n",
    "\n",
    "Basic CNN architecture typically consists of combining the following layers:\n",
    "- convolutions\n",
    "- non-linear activations\n",
    "- pooling\n",
    "\n",
    "And repeating.\n",
    "\n",
    "Final layers are then added on that are tailored to the problem at hand (often fully connected layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/aCNN.jpeg\" alt=\"Drawing\"  width=\"900px\"/>\n",
    "\n",
    "[Credit: Geron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LeNet architecture\n",
    "\n",
    "Convolutional layers first introduced by [Lecun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) in 1998 for digital classification problem.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/lenet-52.png\"  width=\"900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/lenet_summary.png\"  width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### CNN explainer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/cnn_explainer.png\"  width=\"700px\"/>\n",
    "\n",
    "The [CNN explainer](https://poloclub.github.io/cnn-explainer/) is a great visualisation tool, allowing you to look inside a CNN to visualise the layers making up a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), which helped to intitiate the deep learning revolution in 2012, was based on a CNN architecture and showed a significant improvement in performance on the [ImageNet](https://www.image-net.org/) benchmark problem compared to the state-of-the-art at the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/alexnet.png\"  width=\"900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/alexnet_summary.png\"  width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [VGG network](https://arxiv.org/abs/1409.1556) followed soon afterwards, making another significant improvement in performance, while simplifying the architecture.\n",
    "\n",
    "VGG-16 uses 3x3 convolutions only and max pooling layers that step down by a factor of two at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture14_Images/vgg16.png\"  width=\"900px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementing CNNs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Load and set up data\n",
    "\n",
    "Let's consider fashion MNIST again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:28.636573Z",
     "iopub.status.busy": "2022-02-14T21:50:28.635990Z",
     "iopub.status.idle": "2022-02-14T21:50:30.658796Z",
     "shell.execute_reply": "2022-02-14T21:50:30.658033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 21:50:28.794615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-14 21:50:28.794651: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:30.662471Z",
     "iopub.status.busy": "2022-02-14T21:50:30.662264Z",
     "iopub.status.idle": "2022-02-14T21:50:31.566716Z",
     "shell.execute_reply": "2022-02-14T21:50:31.566082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load fashion MNIST data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-30000], X_train_full[-30000:]\n",
    "y_train, y_valid = y_train_full[:-30000], y_train_full[-30000:]\n",
    "\n",
    "# Standardize\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Add final channel axis (one channel)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Plot example data instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:31.570783Z",
     "iopub.status.busy": "2022-02-14T21:50:31.570340Z",
     "iopub.status.idle": "2022-02-14T21:50:31.630155Z",
     "shell.execute_reply": "2022-02-14T21:50:31.629600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Build CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:31.632998Z",
     "iopub.status.busy": "2022-02-14T21:50:31.632808Z",
     "iopub.status.idle": "2022-02-14T21:50:31.681939Z",
     "shell.execute_reply": "2022-02-14T21:50:31.680838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 21:50:31.644594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-14 21:50:31.644626: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-14 21:50:31.644648: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fv-az75-959): /proc/driver/nvidia/version does not exist\n",
      "2022-02-14 21:50:31.644932: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Conv2D(filters=4, kernel_size=7, activation=\"relu\", padding=\"same\", \n",
    "                        input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=8, activation='relu'),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:31.685131Z",
     "iopub.status.busy": "2022-02-14T21:50:31.684714Z",
     "iopub.status.idle": "2022-02-14T21:50:31.692740Z",
     "shell.execute_reply": "2022-02-14T21:50:31.692316Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 4)         200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 4)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 6280      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,570\n",
      "Trainable params: 6,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Compile and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T21:50:31.696018Z",
     "iopub.status.busy": "2022-02-14T21:50:31.695496Z",
     "iopub.status.idle": "2022-02-14T21:51:19.001568Z",
     "shell.execute_reply": "2022-02-14T21:51:19.001004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 16s 16ms/step - loss: 0.7036 - accuracy: 0.7546 - val_loss: 0.5028 - val_accuracy: 0.8250\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.4461 - accuracy: 0.8439 - val_loss: 0.4293 - val_accuracy: 0.8520\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.3963 - accuracy: 0.8613 - val_loss: 0.4071 - val_accuracy: 0.8541\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.4283 - accuracy: 0.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42833212018013, 0.8515999913215637]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "exercise_pointer"
    ]
   },
   "source": [
    "**Exercises:** *You can now complete Exercise 1 in the exercises associated with this lecture.*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
